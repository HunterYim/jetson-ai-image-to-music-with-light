import os
import gc
import csv
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
from transformers import ClapModel, ClapProcessor
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import CLIPModel, CLIPProcessor as HFCLIPProcessor
import serial
import time

# ==========================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜ ë° ê²½ë¡œ
#  - ì‚¬ìš©ìì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
# ==========================================
DATA_ROOT = "/mnt/data"
TEST_IMAGE_PATH = os.path.join(DATA_ROOT, "test_image.jpg")

# ìŒì•… íŒŒì¼, ê²½ë¡œ ë° ì„ë² ë”© íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.
MUSIC_EMB_PATH = os.path.join(DATA_ROOT, "jamendo_clap_filtered2.npy")
META_PATH = os.path.join(DATA_ROOT, "jamendo_clap_filtered2.csv")

# ì‚¬ì „ í•™ìŠµëœ CLIP ê°ì • ë¶„ë¥˜ê¸° ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
CLIP_EMOTION_CKPT = os.path.join(DATA_ROOT, "clip_emotion_classifier.pt")

CLAP_CKPT = "laion/clap-htsat-unfused"
BLIP_CKPT = "Salesforce/blip-image-captioning-base"
CLIP_CKPT = "openai/clip-vit-base-patch32"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
EMO_LABELS = ["amusement","anger","awe","contentment","disgust","excitement","fear","sadness"]

print(f"System Info: Running on {DEVICE}")

# ==========================================
# [ì•„ë‘ì´ë…¸ ì—°ê²° ì„¤ì •]
# ==========================================
ser = None
try:
    ARDUINO_PORT = '/dev/ttyACM0'
    print(f"Arduino ì—°ê²° ì‹œë„ ì¤‘... (í¬íŠ¸: {ARDUINO_PORT})")
    ser = serial.Serial(ARDUINO_PORT, 9600, timeout=1)
    time.sleep(2)
    if ser.is_open:
        print(f"âœ… Arduino ì—°ê²° ì„±ê³µ! í¬íŠ¸ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤.")
    else:
        ser = None
        print(f"âš ï¸ Arduino í¬íŠ¸ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
except serial.SerialException as e:
    ser = None
    print(f"âŒ Arduino ì—°ê²° ì‹¤íŒ¨: {e}")
    print("ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. (LED ì œì–´ ì•ˆ í•¨)")

# ==========================================
# [ê¸°ëŠ¥ 0] ë©”ëª¨ë¦¬ ì •ë¦¬
# ==========================================
def clear_memory():
    torch.cuda.empty_cache()
    gc.collect()

# ==========================================
# [ì•„ë‘ì´ë…¸ LED ì œì–´ ê¸°ëŠ¥]
# ==========================================
def control_light(emotion: str):
    if not ser:
        print("[ì‹œë®¬ë ˆì´ì…˜] LED ì œì–´ ì‹ í˜¸ë¥¼ ë³´ë‚´ì•¼ í•˜ì§€ë§Œ, ì•„ë‘ì´ë…¸ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    command = 'O'
    if emotion == "excitement":    command = 'E'
    elif emotion == "amusement":   command = 'A'
    elif emotion == "awe":         command = 'W'
    elif emotion == "contentment": command = 'C'
    elif emotion == "anger":       command = 'R'
    elif emotion == "fear":        command = 'F'
    elif emotion == "sadness":     command = 'S'
    elif emotion == "disgust":     command = 'D'
    try:
        ser.write(command.encode('utf-8'))
        print(f"[IoT ì œì–´] ê°ì • '{emotion}'ì— ëŒ€í•œ ì‹ í˜¸ '{command}'ë¥¼ ì•„ë‘ì´ë…¸ë¡œ ì „ì†¡í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì•„ë‘ì´ë…¸ë¡œ ì‹ í˜¸ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ==========================================
# [ê¸°ëŠ¥ 1] CLIP ê°ì • ë¶„ì„
# ==========================================
class CLIPEmotionHead(nn.Module):
    def __init__(self, in_dim=512, num_classes=8):
        super().__init__()
        self.classifier = nn.Linear(in_dim, num_classes)
    def forward(self, x):
        return self.classifier(x)

def get_emotion(pil_img):
    print("\n-------------------------------------------")
    print("[Step 1] Loading CLIP for Emotion Analysis...")
    emotion = "neutral" # ê¸°ë³¸ê°’ ì„¤ì •
    try:
        clip_backbone = CLIPModel.from_pretrained(CLIP_CKPT).to(DEVICE).eval()
        clip_proc = HFCLIPProcessor.from_pretrained(CLIP_CKPT)
        emotion_head = CLIPEmotionHead(num_classes=len(EMO_LABELS)).to(DEVICE)

        if os.path.exists(CLIP_EMOTION_CKPT):
            ckpt = torch.load(CLIP_EMOTION_CKPT, map_location=DEVICE)
            state_dict = ckpt["state_dict"] if "state_dict" in ckpt else ckpt
            emotion_head.load_state_dict(state_dict, strict=False)
            emotion_head.eval()
            print("âœ… Custom Emotion Checkpoint Loaded.")
        else:
            print("âš ï¸ Checkpoint not found. Using random weights.")

        inputs = clip_proc(images=pil_img, return_tensors="pt").to(DEVICE)
        with torch.no_grad():
            img_feats = clip_backbone.get_image_features(**inputs)
            img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)
            logits = emotion_head(img_feats)
            probs = torch.softmax(logits, dim=-1)[0]

        top_id = int(torch.argmax(probs).item())
        emotion = EMO_LABELS[top_id]
        prob = float(probs[top_id].item())

        print(f"âœ… Detected: {emotion.upper()} ({prob*100:.1f}%)")

    except Exception as e:
        print(f"âŒ Error in CLIP: {e}")
    finally:
        # ì˜¤ë¥˜ ë°œìƒ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰
        del clip_backbone, emotion_head, clip_proc
        clear_memory()
        return emotion

# ==========================================
# [ê¸°ëŠ¥ 2] BLIP ìº¡ì…˜ ìƒì„±
# ==========================================
def get_caption(pil_img):
    print("\n-------------------------------------------")
    print("[Step 2] Loading BLIP for Captioning...")
    caption = "an image" # ê¸°ë³¸ê°’ ì„¤ì •
    try:
        processor = BlipProcessor.from_pretrained(BLIP_CKPT)
        model = BlipForConditionalGeneration.from_pretrained(BLIP_CKPT).to(DEVICE).eval()
        inputs = processor(pil_img, return_tensors="pt").to(DEVICE)
        with torch.no_grad():
            out = model.generate(**inputs, max_new_tokens=50)
            caption = processor.decode(out[0], skip_special_tokens=True)
        print(f"âœ… Caption: \"{caption}\"")
    except Exception as e:
        print(f"âŒ Error in BLIP: {e}")
    finally:
        del model, processor
        clear_memory()
        return caption

# ==========================================
# [ê¸°ëŠ¥ 3] CLAP ìŒì•… ì¶”ì²œ
# ==========================================
def search_music(query_text):
    print("\n-------------------------------------------")
    print("[Step 3] Loading CLAP & DB for Music Search...")

    if not os.path.exists(MUSIC_EMB_PATH) or not os.path.exists(META_PATH):
        print("âŒ Error: DB Files missing!")
        return

    try:
        # 1. DB ë¡œë“œ (CPU ë©”ëª¨ë¦¬ ì‚¬ìš©)
        song_embeds_cpu = torch.from_numpy(np.load(MUSIC_EMB_PATH)).float()
        song_embeds_cpu = song_embeds_cpu / (song_embeds_cpu.norm(dim=-1, keepdim=True) + 1e-8)

        song_meta = []
        with open(META_PATH, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                song_meta.append(row)
        print(f"âœ… Loaded {len(song_meta)} songs from DB.")

        # 2. ëª¨ë¸ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì„ë² ë”© (GPU ì‚¬ìš©)
        model = ClapModel.from_pretrained(CLAP_CKPT).to(DEVICE).eval()
        processor = ClapProcessor.from_pretrained(CLAP_CKPT)
        print(f"Final Query: '{query_text}'")
        inputs = processor(text=[query_text], return_tensors="pt", padding=True).to(DEVICE)
        with torch.no_grad():
            text_emb = model.get_text_features(**inputs)
            text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)

        # 3. ìœ ì‚¬ë„ ê³„ì‚°
        sims = text_emb @ song_embeds_cpu.to(DEVICE).T

        # 4. Top 3 ê²°ê³¼ ì¶œë ¥
        scores, idx = sims[0].topk(3)

        print("\nğŸµ Top 3 Recommended Songs:")
        for i, score in zip(idx.cpu().numpy(), scores.cpu().numpy()):
            row = song_meta[i]
            # CSV íŒŒì¼ì˜ 'path' ì»¬ëŸ¼ì„ ì œëª©ì²˜ëŸ¼ ì‚¬ìš©
            path_title = row.get("path") or "Unknown Path"
            print(f"  - [{score:.4f}] {path_title}")

    except Exception as e:
        print(f"âŒ Error in CLAP/Music-Search: {e}")
    finally:
        del model, processor, text_emb, sims, song_embeds_cpu
        clear_memory()

# ==========================================
# [Main] ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ==========================================
if __name__ == "__main__":
    if not os.path.exists(TEST_IMAGE_PATH):
        print(f"âŒ Error: '{TEST_IMAGE_PATH}' not found.")
    else:
        try:
            print(f"âœ… Processing image: {TEST_IMAGE_PATH}")
            image = Image.open(TEST_IMAGE_PATH).convert("RGB")

            emotion_result = get_emotion(image)
            control_light(emotion_result)
            caption_result = get_caption(image)
            
            search_music(f"{emotion_result} mood. {caption_result}")

            print("\nâœ… All Pipeline Finished Successfully!")

        except KeyboardInterrupt:
             print("\n\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        finally:
             if ser and ser.is_open:
                 ser.close()
def clear_memory():
    torch.cuda.empty_cache()
    gc.collect()

def control_light(emotion: str):
    if not ser: return
    command = 'O'
    if emotion == "excitement":    command = 'E'
    elif emotion == "amusement":   command = 'A'
    elif emotion == "awe":         command = 'W'
    elif emotion == "contentment": command = 'C'
    elif emotion == "anger":       command = 'R'
    elif emotion == "fear":        command = 'F'
    elif emotion == "sadness":     command = 'S'
    elif emotion == "disgust":     command = 'D'
    try:
        ser.write(command.encode('utf-8'))
        print(f"ğŸ’¡ [IoT ì œì–´] ê°ì • '{emotion}'ì— ëŒ€í•œ ì‹ í˜¸ '{command}'ë¥¼ ì•„ë‘ì´ë…¸ë¡œ ì „ì†¡í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì•„ë‘ì´ë…¸ë¡œ ì‹ í˜¸ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

class CLIPEmotionHead(nn.Module):
    def __init__(self, in_dim=512, num_classes=8):
        super().__init__()
        self.classifier = nn.Linear(in_dim, num_classes)
    def forward(self, x): return self.classifier(x)

def get_emotion(pil_img):
    print("\n-------------------------------------------")
    print("[Step 1] Loading CLIP for Emotion Analysis...")
    emotion = "neutral"
    try:
        clip_backbone = CLIPModel.from_pretrained(CLIP_CKPT).to(DEVICE).eval()
        clip_proc = HFCLIPProcessor.from_pretrained(CLIP_CKPT)
        emotion_head = CLIPEmotionHead(num_classes=len(EMO_LABELS)).to(DEVICE)
        if os.path.exists(CLIP_EMOTION_CKPT):
            ckpt = torch.load(CLIP_EMOTION_CKPT, map_location=DEVICE)
            state_dict = ckpt["state_dict"] if "state_dict" in ckpt else ckpt
            emotion_head.load_state_dict(state_dict, strict=False)
            emotion_head.eval()
        inputs = clip_proc(images=pil_img, return_tensors="pt").to(DEVICE)
        with torch.no_grad():
            img_feats = clip_backbone.get_image_features(**inputs)
            img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)
            logits = emotion_head(img_feats)
            probs = torch.softmax(logits, dim=-1)[0]
        top_id = int(torch.argmax(probs).item())
        emotion = EMO_LABELS[top_id]
    except Exception as e:
        print(f"âŒ Error in CLIP: {e}")
    finally:
        del clip_backbone, emotion_head, clip_proc
        clear_memory()
        return emotion

def get_caption(pil_img):
    print("\n-------------------------------------------")
    print("[Step 2] Loading BLIP for Captioning...")
    caption = "an image"
    try:
        processor = BlipProcessor.from_pretrained(BLIP_CKPT)
        model = BlipForConditionalGeneration.from_pretrained(BLIP_CKPT).to(DEVICE).eval()
        inputs = processor(pil_img, return_tensors="pt").to(DEVICE)
        with torch.no_grad():
            out = model.generate(**inputs, max_new_tokens=50)
            caption = processor.decode(out[0], skip_special_tokens=True)
    except Exception as e:
        print(f"âŒ Error in BLIP: {e}")
    finally:
        del model, processor
        clear_memory()
        return caption

def search_music(query_text):
    print("\n-------------------------------------------")
    print("[Step 3] Loading CLAP & DB for Music Search...")
    if not os.path.exists(MUSIC_EMB_PATH) or not os.path.exists(META_PATH):
        print("âŒ Error: DB Files missing!")
        return
    try:
        song_embeds_cpu = torch.from_numpy(np.load(MUSIC_EMB_PATH)).float()
        song_embeds_cpu = song_embeds_cpu / (song_embeds_cpu.norm(dim=-1, keepdim=True) + 1e-8)
        song_meta = []
        with open(META_PATH, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                song_meta.append(row)
        model = ClapModel.from_pretrained(CLAP_CKPT).to(DEVICE).eval()
        processor = ClapProcessor.from_pretrained(CLAP_CKPT)
        inputs = processor(text=[query_text], return_tensors="pt", padding=True).to(DEVICE)
        with torch.no_grad():
            text_emb = model.get_text_features(**inputs)
            text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)
        sims = text_emb @ song_embeds_cpu.to(DEVICE).T
        scores, idx = sims[0].topk(3)
        print("\nğŸµ Top 3 Recommended Songs:")
        for i, score in zip(idx.cpu().numpy(), scores.cpu().numpy()):
            row = song_meta[i]
            path_title = row.get("path") or "Unknown Path"
            print(f"  - [{score:.4f}] {path_title}")
    except Exception as e:
        print(f"âŒ Error in CLAP/Music-Search: {e}")
    finally:
        del model, processor, text_emb, sims, song_embeds_cpu
        clear_memory()

if __name__ == "__main__":
    if not os.path.exists(TEST_IMAGE_PATH):
        print(f"âŒ Error: '{TEST_IMAGE_PATH}' not found.")
    else:
        try:
            image = Image.open(TEST_IMAGE_PATH).convert("RGB")
            emotion_result = get_emotion(image)
            control_light(emotion_result)
            caption_result = get_caption(image)
            search_music(f"{emotion_result} mood. {caption_result}")
            print("\nâœ… All Pipeline Finished Successfully!")
        except KeyboardInterrupt:
             print("\n\nğŸ›‘ User interrupted the program.")
        finally:
             if ser and ser.is_open:
                 ser.close()
